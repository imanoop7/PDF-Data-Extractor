{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf4llm\n",
      "  Downloading pymupdf4llm-0.0.17-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pymupdf>=1.24.10 (from pymupdf4llm)\n",
      "  Downloading PyMuPDF-1.24.11-cp38-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf4llm-0.0.17-py3-none-any.whl (26 kB)\n",
      "Downloading PyMuPDF-1.24.11-cp38-abi3-win_amd64.whl (16.0 MB)\n",
      "   ---------------------------------------- 0.0/16.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/16.0 MB 6.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.4/16.0 MB 7.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.2/16.0 MB 8.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 6.0/16.0 MB 8.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.8/16.0 MB 7.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.6/16.0 MB 6.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.4/16.0 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 9.2/16.0 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.7/16.0 MB 5.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.5/16.0 MB 5.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.3/16.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 12.1/16.0 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.6/16.0 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.6/16.0 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.4/16.0 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/16.0 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/16.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/16.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/16.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/16.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/16.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.0/16.0 MB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf, pymupdf4llm\n",
      "Successfully installed pymupdf-1.24.11 pymupdf4llm-0.0.17\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=[==                                      ] ( 1/19=[====                                    ] ( 2/19=[======                                  ] ( 3/19=[========                                ] ( 4/19=[==========                              ] ( 5/19=[============                            ] ( 6/19=[==============                          ] ( 7/19=[================                        ] ( 8/19=[==================                      ] ( 9/19==[=====================                   ] (10/1=[=======================                 ] (11/1=[=========================               ] (12/1=[===========================             ] (13/1=[=============================           ] (14/1=[===============================         ] (15/1=[=================================       ] (16/1=[===================================     ] (17/1=[=====================================   ] (18/1=[========================================] (19/19]\n",
      "## A Neural Probabilistic Language Model\n",
      "\n",
      "**Yoshua Bengio** BENGIOY@IRO.UMONTREAL.CA\n",
      "**Réjean Ducharme** DUCHARME@IRO.UMONTREAL.CA\n",
      "**Pascal Vincent** VINCENTP@IRO.UMONTREAL.CA\n",
      "**Christian Jauvin** JAUVINC@IRO.UMONTREAL.CA\n",
      "_Département d’Informatique et Recherche Opérationnelle_\n",
      "_Centre de Recherche Mathématiques_\n",
      "_Université de Montréal, Montréal, Québec, Canada_\n",
      "\n",
      "**Editors: Jaz Kandola, Thomas Hofmann, Tomaso Poggio and John Shawe-Taylor**\n",
      "\n",
      "### Abstract\n",
      "\n",
      "A goal of statistical language modeling \n"
     ]
    }
   ],
   "source": [
    "# Use Case 1: Basic Markdown Extraction\n",
    "import pymupdf4llm\n",
    "\n",
    "# Extract PDF content as Markdown\n",
    "md_text = pymupdf4llm.to_markdown(\"bengio03a.pdf\")\n",
    "print(md_text[:500])  # Print first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/2===================[====================                    ] (1/2===================[========================================] (2/2]\n",
      "processors. Then the K backward phases are initiated, to obtain the K partial gradient vectors [∂][L]\n",
      "\n",
      "∂a\n",
      "and [∂][L]\n",
      "\n",
      "∂x [. After exchanging these gradient vectors among the processors, each processor can complete]\n",
      "the backward phase and update parameters. This method mainly saves time because of the savings\n",
      "in network communication latency (the amount of data transferred is the same). It may lose in convergence time if K is too large, for the same reason that batch gradient descent is generally\n"
     ]
    }
   ],
   "source": [
    "# Use Case 2: Extracting specific pages\n",
    "import pymupdf4llm\n",
    "\n",
    "# Extract only pages 0 and 1 (first two pages)\n",
    "md_text = pymupdf4llm.to_markdown(\"bengio03a.pdf\", pages=[10, 11])\n",
    "print(md_text[:500])  # Print first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=[==                                      ] ( 1/19=[====                                    ] ( 2/19=[======                                  ] ( 3/19=[========                                ] ( 4/19=[==========                              ] ( 5/19=[============                            ] ( 6/19=[==============                          ] ( 7/19=[================                        ] ( 8/19=[==================                      ] ( 9/19==[=====================                   ] (10/1=[=======================                 ] (11/1=[=========================               ] (12/1=[===========================             ] (13/1=[=============================           ] (14/1=[===============================         ] (15/1=[=================================       ] (16/1=[===================================     ] (17/1=[=====================================   ] (18/1=[========================================] (19/19]\n",
      "Markdown saved to output.md\n"
     ]
    }
   ],
   "source": [
    "# Use Case 3: Saving Markdown to a file\n",
    "import pymupdf4llm\n",
    "import pathlib\n",
    "\n",
    "md_text = pymupdf4llm.to_markdown(\"bengio03a.pdf\")\n",
    "pathlib.Path(\"output.md\").write_bytes(md_text.encode())\n",
    "print(\"Markdown saved to output.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.11.18-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index)\n",
      "  Downloading llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.12.0,>=0.11.18 (from llama-index)\n",
      "  Downloading llama_index_core-0.11.18-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.3.0,>=0.2.10 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.2.15-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.2.2-py3-none-any.whl.metadata (678 bytes)\n",
      "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.43.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.18->llama-index) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (3.10.2)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (0.6.6)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (1.2.14)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.18->llama-index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (2024.6.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\anoop maurya\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (3.3)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (10.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.18->llama-index) (1.16.0)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n",
      "  Downloading llama_cloud-0.1.4-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: pandas in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.3.1)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index)\n",
      "  Downloading llama_parse-0.5.10-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index) (2023.12.25)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.18->llama-index) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.18->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.18->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.18->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.18->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.18->llama-index) (1.9.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.18->llama-index) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.18->llama-index) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.18->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.18->llama-index) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.18->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.18->llama-index) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anoop maurya\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk>3.8.1->llama-index) (0.4.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (0.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.18->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.18->llama-index) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.18->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.18->llama-index) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.18->llama-index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.18->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.18->llama-index) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anoop maurya\\appdata\\roaming\\python\\python312\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\anoop maurya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\anoop maurya\\appdata\\roaming\\python\\python312\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.18->llama-index) (24.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anoop maurya\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
      "Downloading llama_index-0.11.18-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_core-0.11.18-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 1.0/1.6 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 7.0 MB/s eta 0:00:00\n",
      "Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl (10 kB)\n",
      "Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 15.1 MB/s eta 0:00:00\n",
      "Downloading llama_index_llms_openai-0.2.15-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.2.2-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading llama_cloud-0.1.4-py3-none-any.whl (176 kB)\n",
      "Downloading llama_parse-0.5.10-py3-none-any.whl (12 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: striprtf, dirtyjson, nltk, llama-index-core, llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.8.1\n",
      "    Uninstalling nltk-3.8.1:\n",
      "      Successfully uninstalled nltk-3.8.1\n",
      "Successfully installed dirtyjson-1.0.8 llama-cloud-0.1.4 llama-index-0.11.18 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-core-0.11.18 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.4.0 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.2.15 llama-index-multi-modal-llms-openai-0.2.2 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.2 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.10 nltk-3.9.1 striprtf-0.0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "graphrag 0.1.1 requires lancedb<0.10.0,>=0.9.0, but you have lancedb 0.5.7 which is incompatible.\n",
      "graphrag 0.1.1 requires nltk==3.8.1, but you have nltk 3.9.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# required llama-index\n",
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Number of LlamaIndex documents: 19\n",
      "Content of first document: ## A Neural Probabilistic Language Model\n",
      "\n",
      "**Yoshua Bengio** BENGIOY@IRO.UMONTREAL.CA\n",
      "**Réjean Ducharme** DUCHARME@IRO.UMONTREAL.CA\n",
      "**Pascal Vincent** VINCENTP@IRO.UMONTREAL.CA\n",
      "**Christian Jauvin** JAUVINC@IRO.UMONTREAL.CA\n",
      "_Département d’Informatique et Recherche Opérationnelle_\n",
      "_Centre de Recherche Mathématiques_\n",
      "_Université de Montréal, Montréal, Québec, Canada_\n",
      "\n",
      "**Editors: Jaz Kandola, Thomas Hofmann, Tomaso Poggio and John Shawe-Taylor**\n",
      "\n",
      "### Abstract\n",
      "\n",
      "A goal of statistical language modeling \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use Case 4: Extracting as LlamaIndex document\n",
    "import pymupdf4llm\n",
    "\n",
    "llama_reader = pymupdf4llm.LlamaMarkdownReader()\n",
    "llama_docs = llama_reader.load_data(\"bengio03a.pdf\")\n",
    "print(f\"Number of LlamaIndex documents: {len(llama_docs)}\")\n",
    "print(f\"Content of first document: {llama_docs[0].text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/2===================[====================                    ] (1/2===================[========================================] (2/2]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Use Case 5: Image Extraction\n",
    "md_text_images = pymupdf4llm.to_markdown(doc=\"bengio03a.pdf\",\n",
    "                                         pages=[1, 11],\n",
    "                                         page_chunks=True,\n",
    "                                         write_images=True,\n",
    "                                         image_path=\"images\",\n",
    "                                         image_format=\"jpg\",\n",
    "                                         dpi=200)\n",
    "print(md_text_images[0]['images'])  # Print image information from the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/3============[=============                           ] (1/============[==========================              ] (2/3=============[========================================] (3/3]\n",
      "{'metadata': {'format': 'PDF 1.1', 'title': 'bengio03a.dvi', 'author': '', 'subject': '', 'keywords': '', 'creator': 'dvips(k) 5.86 Copyright 1999 Radical Eye Software', 'producer': 'Acrobat Distiller Command 3.01 for Solaris 2.3 and later (SPARC)', 'creationDate': 'D:191030226135614', 'modDate': '', 'trapped': '', 'encryption': None, 'file_path': 'bengio03a.pdf', 'page_count': 19, 'page': 1}, 'toc_items': [], 'tables': [], 'images': [], 'graphics': [], 'text': '## A Neural Probabilistic Language Model\\n\\n**Yoshua Bengio** BENGIOY@IRO.UMONTREAL.CA\\n**Réjean Ducharme** DUCHARME@IRO.UMONTREAL.CA\\n**Pascal Vincent** VINCENTP@IRO.UMONTREAL.CA\\n**Christian Jauvin** JAUVINC@IRO.UMONTREAL.CA\\n_Département d’Informatique et Recherche Opérationnelle_\\n_Centre de Recherche Mathématiques_\\n_Université de Montréal, Montréal, Québec, Canada_\\n\\n**Editors: Jaz Kandola, Thomas Hofmann, Tomaso Poggio and John Shawe-Taylor**\\n\\n### Abstract\\n\\nA goal of statistical language modeling is to learn the joint probability function of sequences of\\nwords in a language. This is intrinsically difficult because of the curse of dimensionality: a word\\nsequence on which the model will be tested is likely to be different from all the word sequences seen\\nduring training. Traditional but very successful approaches based on n-grams obtain generalization\\nby concatenating very short overlapping sequences seen in the training set. We propose to fight the\\ncurse of dimensionality by learning a distributed representation for words which allows each\\ntraining sentence to inform the model about an exponential number of semantically neighboring\\nsentences. The model learns simultaneously (1) a distributed representation for each word along\\nwith (2) the probability function for word sequences, expressed in terms of these representations.\\nGeneralization is obtained because a sequence of words that has never been seen before gets high\\nprobability if it is made of words that are similar (in the sense of having a nearby representation) to\\nwords forming an already seen sentence. Training such large models (with millions of parameters)\\nwithin a reasonable time is itself a significant challenge. We report on experiments using neural\\nnetworks for the probability function, showing on two text corpora that the proposed approach\\nsignificantly improves on state-of-the-art n-gram models, and that the proposed approach allows to\\ntake advantage of longer contexts.\\n\\n**Keywords:** Statistical language modeling, artificial neural networks, distributed representation,\\ncurse of dimensionality\\n\\n### 1. Introduction\\n\\nA fundamental problem that makes language modeling and other learning problems difficult is the\\n_curse of dimensionality. It is particularly obvious in the case when one wants to model the joint_\\ndistribution between many discrete random variables (such as words in a sentence, or discrete attributes in a data-mining task). For example, if one wants to model the joint distribution of 10\\nconsecutive words in a natural language with a vocabulary V of size 100,000, there are potentially\\n100000[10] 1 = 10[50] 1 free parameters. When modeling continuous variables, we obtain gen_−_ _−_\\neralization more easily (e.g. with smooth classes of functions like multi-layer neural networks or\\nGaussian mixture models) because the function to be learned can be expected to have some local smoothness properties. For discrete spaces, the generalization structure is not as obvious: any\\nchange of these discrete variables may have a drastic impact on the value of the function to be esti\\n_⃝c_ 2003 Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin.\\n\\n\\n-----\\n\\n', 'words': []}\n"
     ]
    }
   ],
   "source": [
    "# Use Case 6: Chunking with Metadata\n",
    "md_text_chunks = pymupdf4llm.to_markdown(doc=\"bengio03a.pdf\",\n",
    "                                         pages=[0, 1, 2],\n",
    "                                         page_chunks=True)\n",
    "print(md_text_chunks[0])  # Print the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bengio03a.pdf...\n",
      "[                                        ] (0/2===================[====================                    ] (1/2===================[========================================] (2/2]\n",
      "[(90.0, 93.28394317626953, 119.24702453613281, 104.19293975830078, 'mated,', 0, 0, 0), (122.51972961425781, 93.28394317626953, 138.18504333496094, 104.19293975830078, 'and', 0, 0, 1), (141.37046813964844, 93.28394317626953, 164.95570373535156, 104.19293975830078, 'when', 0, 0, 2), (168.14112854003906, 93.28394317626953, 181.3955535888672, 104.19293975830078, 'the', 0, 0, 3), (184.59188842773438, 93.28394317626953, 217.7770538330078, 104.19293975830078, 'number', 0, 0, 4)]\n"
     ]
    }
   ],
   "source": [
    "# Use Case 7: Word Extraction\n",
    "md_text_words = pymupdf4llm.to_markdown(doc=\"bengio03a.pdf\",\n",
    "                                        pages=[1,2],\n",
    "                                        page_chunks=True,\n",
    "                                        write_images=True,\n",
    "                                        image_path=\"images\",\n",
    "                                        image_format=\"jpg\",\n",
    "                                        dpi=200,\n",
    "                                        extract_words=True)\n",
    "print(md_text_words[0]['words'][:5])  # Print the first 5 words from the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2210.03629v3.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n"
     ]
    }
   ],
   "source": [
    "# Use Case 5: Table Extraction\n",
    "import pymupdf4llm\n",
    "import json\n",
    "\n",
    "md_text_tables = pymupdf4llm.to_markdown(doc=\"2408.14717v1.pdf\",\n",
    "                                         pages=[5],  # Specify pages containing tables\n",
    "                                         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Col1|Type Definition ReAct CoT|\n",
      "|---|---|\n",
      "\n",
      "|Col1|Type Definition ReAct CoT|\n",
      "|---|---|\n",
      "|Success|True positive Correct reasoning trace and facts 94% 86% False positive Hallucinated reasoning trace or facts 6% 14%|\n",
      "|Failure|Reasoning error Wrong reasoning trace (including failing to recover from repetitive steps) 47% 16% Search result error Search return empty or does not contain useful information 23% - Hallucination Hallucinated reasoning trace or facts 0% 56% Label ambiguity Right prediction but did not match the label precisely 29% 28%|\n",
      "\n",
      "\n",
      "Table 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their\n",
      "percentages in randomly selected examples studied by human.\n",
      "\n",
      "**ReAct vs. CoT** On the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly\n",
      "lags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only\n",
      "differ by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge\n",
      "is vital. To better understand the behavioral difference between ReAct and CoT on HotpotQA, we\n",
      "randomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct\n",
      "and CoT respectively (thus 200 examples in total), and manually labeled their success and failure\n",
      "modes in Table 2. Some key observations are as follows:\n",
      "\n",
      "A) Hallucination is a serious problem for CoT, resulting in much higher false positive rate than\n",
      "ReAct (14% vs. 6%) in success mode, and make up its major failure mode (56%). In contrast, the\n",
      "problem solving trajectory of ReActis more grounded, fact-driven, and trustworthy, thanks to the\n",
      "access of an external knowledge base.\n",
      "\n",
      "B) While interleaving reasoning, action and observation steps improves ReAct’s grounded**ness and trustworthiness, such a structural constraint also reduces its flexibility in formulating**\n",
      "**reasoning steps, leading to more reasoning error rate than CoT. we note that there is one frequent**\n",
      "error pattern specific to ReAct, in which the model repetitively generates the previous thoughts and\n",
      "actions, and we categorize it as part of “reasoning error” as the model fails to reason about what the\n",
      "proper next action to take and jump out of the loop[4].\n",
      "\n",
      "C) For ReAct, successfully retrieving informative knowledge via search is critical. Noninformative search, which counts for 23% of the error cases, derails the model reasoning and gives\n",
      "it a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between\n",
      "factuality and flexibility, which motivates our proposed strategies of combining two methods.\n",
      "\n",
      "We provide examples for each success and failure modes in Appendix E.1. We also find some\n",
      "HotpotQA questions may contain outdated answer labels, see Figure 4 for example.\n",
      "\n",
      "**ReAct + CoT-SC perform best for prompting LLMs** Also shown in Table 1, the best prompting\n",
      "method on HotpotQA and Fever are ReAct CoT-SC and CoT-SC ReAct respectively.\n",
      "_→_ _→_\n",
      "Furthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC\n",
      "samples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both\n",
      "significantly and consistently outperform CoT-SC across different number of samples, reaching\n",
      "CoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of\n",
      "properly combining model internal knowledge and external knowledge for reasoning tasks.\n",
      "\n",
      "**ReAct performs best for fine-tuning** Figure 3 shows the scaling effect of prompting/finetuning\n",
      "four methods (Standard, CoT, Act, ReAct) on HotpotQA. With PaLM-8/62B, prompting ReAct\n",
      "performs worst among four methods due to the difficulty to learn both reasoning and acting from\n",
      "in-context examples. However, when finetuned with just 3,000 examples, ReAct becomes the best\n",
      "method among the four, with PaLM-8B finetuned ReAct outperforming all PaLM-62B prompting\n",
      "methods, and PaLM-62B finetuned ReAct outperforming all 540B prompting methods. In contrast,\n",
      "finetuning Standard or CoT is significantly worse than finetuning ReAct or Act for both PaLM8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge\n",
      "facts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a\n",
      "more generalizable skill for knowledge reasoning. As all prompting methods are still significantly\n",
      "far from domain-specific state-of-the-art approaches (Table 1), we believe finetuning with more\n",
      "human-written data might be a better way to unleash the power of ReAct.\n",
      "\n",
      "4We suspect that this could be due to the sub-optimal greedy decoding procedure, and future work using\n",
      "better decoding (e.g. beam search) might help address this issue.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(md_text_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
